{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi !\n",
      "Bot: Hello!\n",
      "User: who are you ?\n",
      "Bot: I'm a chatbot designed to assist you!\n",
      "User: I want to ride a horse\n",
      "Bot: Horses require regular grooming, balanced nutrition, and exercise to stay healthy.\n",
      "User: wanna play basketball\n",
      "Bot: Notable players are Michael Jordan, LeBron James, and Kobe Bryant.\n",
      "User: bye\n",
      "Bot: Take care!\n"
     ]
    }
   ],
   "source": [
    "%run v1.py # Bag of words with simple Pytorch Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delah\\Documents\\GitHub\\AI-Chatbot\\v2.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello !\n",
      "Bot: Hello!\n",
      "User: I feel bad\n",
      "Bot: I'm sorry to hear that. Can you describe the issue?\n",
      "User: I want to adopt a Lion\n",
      "Bot: Lions live in groups called prides and primarily inhabit savannas and grasslands.\n",
      "User: or maybe a zebra\n",
      "Bot: They inhabit the grasslands and savannas of Africa.\n",
      "User: I also play video Games\n",
      "Bot: Gaming consoles include PlayStation, Xbox, Nintendo Switch, and PC.\n",
      "User: I want to read a book\n",
      "Bot: You might enjoy 'To Kill a Mockingbird', '1984', or 'The Great Gatsby'.\n",
      "User: or maybe watch a movie\n",
      "Bot: Some great movies include The Dark Knight, Forrest Gump, and Interstellar.\n",
      "User: goodbye !\n",
      "Bot: Bye! Come back soon!\n"
     ]
    }
   ],
   "source": [
    "%run v2.py # SBERT and simple Pytorch Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\delah\\anaconda3\\envs\\GPTK\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\delah\\anaconda3\\envs\\GPTK\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "%run v3.py # SBERT and simple Tensorflow Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Choose a mode:\n",
      "1: Load the model for evaluation\n",
      "2: Retrain the model from scratch\n",
      "3: Continue training from a checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delah\\Documents\\GitHub\\AI-Chatbot\\gpt.py:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from iteration 1999...\n",
      "step 2000: train loss 1.4844, val loss 1.7322\n",
      "step 2200: train loss 1.4467, val loss 1.6942\n",
      "step 2400: train loss 1.4294, val loss 1.6668\n",
      "step 2600: train loss 1.4027, val loss 1.6419\n",
      "step 2800: train loss 1.3854, val loss 1.6345\n",
      "step 3000: train loss 1.3636, val loss 1.6182\n",
      "step 3200: train loss 1.3611, val loss 1.6237\n",
      "step 3400: train loss 1.3383, val loss 1.5967\n",
      "step 3600: train loss 1.3220, val loss 1.5791\n",
      "step 3800: train loss 1.3180, val loss 1.5876\n",
      "step 4000: train loss 1.2982, val loss 1.5693\n",
      "step 4200: train loss 1.2853, val loss 1.5571\n",
      "step 4400: train loss 1.2738, val loss 1.5524\n",
      "step 4600: train loss 1.2699, val loss 1.5593\n",
      "step 4800: train loss 1.2581, val loss 1.5487\n",
      "step 5000: train loss 1.2481, val loss 1.5424\n",
      "step 5200: train loss 1.2393, val loss 1.5357\n",
      "step 5400: train loss 1.2331, val loss 1.5358\n",
      "step 5600: train loss 1.2236, val loss 1.5306\n",
      "step 5800: train loss 1.2125, val loss 1.5319\n",
      "step 6000: train loss 1.2081, val loss 1.5207\n",
      "step 6200: train loss 1.2053, val loss 1.5274\n",
      "step 6400: train loss 1.1979, val loss 1.5192\n",
      "step 6600: train loss 1.1889, val loss 1.5154\n",
      "step 6800: train loss 1.1818, val loss 1.5189\n",
      "step 7000: train loss 1.1728, val loss 1.5170\n",
      "step 7200: train loss 1.1679, val loss 1.5132\n",
      "step 7400: train loss 1.1668, val loss 1.5113\n",
      "step 7600: train loss 1.1609, val loss 1.5105\n",
      "step 7800: train loss 1.1553, val loss 1.5113\n",
      "step 8000: train loss 1.1477, val loss 1.5036\n",
      "step 8200: train loss 1.1435, val loss 1.5124\n",
      "step 8400: train loss 1.1342, val loss 1.4970\n",
      "step 8600: train loss 1.1327, val loss 1.5088\n",
      "step 8800: train loss 1.1277, val loss 1.5057\n",
      "step 9000: train loss 1.1223, val loss 1.4973\n",
      "step 9200: train loss 1.1239, val loss 1.4961\n",
      "step 9400: train loss 1.1114, val loss 1.5018\n",
      "step 9600: train loss 1.1137, val loss 1.5075\n",
      "step 9800: train loss 1.1069, val loss 1.4971\n",
      "step 10000: train loss 1.0999, val loss 1.5037\n",
      "step 10200: train loss 1.0986, val loss 1.5036\n",
      "step 10400: train loss 1.0939, val loss 1.5142\n",
      "step 10600: train loss 1.0891, val loss 1.4985\n",
      "step 10800: train loss 1.0821, val loss 1.4961\n",
      "step 11000: train loss 1.0783, val loss 1.5056\n",
      "step 11200: train loss 1.0762, val loss 1.5009\n",
      "step 11400: train loss 1.0762, val loss 1.5140\n",
      "step 11600: train loss 1.0705, val loss 1.5041\n",
      "step 11800: train loss 1.0629, val loss 1.5056\n",
      "step 12000: train loss 1.0599, val loss 1.5008\n",
      "step 12200: train loss 1.0559, val loss 1.4979\n",
      "step 12400: train loss 1.0531, val loss 1.5039\n",
      "step 12600: train loss 1.0508, val loss 1.4991\n",
      "step 12800: train loss 1.0446, val loss 1.4983\n",
      "step 13000: train loss 1.0424, val loss 1.4992\n",
      "step 13200: train loss 1.0359, val loss 1.4989\n",
      "step 13400: train loss 1.0354, val loss 1.5074\n",
      "step 13600: train loss 1.0299, val loss 1.4940\n",
      "step 13800: train loss 1.0291, val loss 1.4938\n",
      "step 14000: train loss 1.0261, val loss 1.5127\n",
      "step 14200: train loss 1.0210, val loss 1.5016\n",
      "step 14400: train loss 1.0165, val loss 1.4975\n",
      "step 14600: train loss 1.0158, val loss 1.5205\n",
      "step 14800: train loss 1.0146, val loss 1.5125\n",
      "step 15000: train loss 1.0075, val loss 1.5200\n",
      "step 15200: train loss 1.0112, val loss 1.5179\n",
      "step 15400: train loss 1.0028, val loss 1.5069\n",
      "step 15600: train loss 1.0022, val loss 1.5210\n",
      "step 15800: train loss 1.0005, val loss 1.5142\n",
      "step 16000: train loss 0.9939, val loss 1.5115\n",
      "step 16200: train loss 0.9877, val loss 1.5137\n",
      "step 16400: train loss 0.9871, val loss 1.5091\n",
      "step 16600: train loss 0.9866, val loss 1.5102\n",
      "step 16800: train loss 0.9847, val loss 1.5113\n",
      "step 17000: train loss 0.9767, val loss 1.5078\n",
      "step 17200: train loss 0.9770, val loss 1.5128\n",
      "step 17400: train loss 0.9739, val loss 1.5231\n",
      "step 17600: train loss 0.9662, val loss 1.5095\n",
      "step 17800: train loss 0.9679, val loss 1.5051\n",
      "step 18000: train loss 0.9637, val loss 1.5234\n",
      "step 18200: train loss 0.9618, val loss 1.5183\n",
      "step 18400: train loss 0.9561, val loss 1.5136\n",
      "step 18600: train loss 0.9541, val loss 1.5221\n",
      "step 18800: train loss 0.9527, val loss 1.5194\n",
      "step 19000: train loss 0.9487, val loss 1.5178\n",
      "step 19200: train loss 0.9537, val loss 1.5365\n",
      "step 19400: train loss 0.9507, val loss 1.5401\n",
      "step 19600: train loss 0.9383, val loss 1.5288\n",
      "step 19800: train loss 0.9422, val loss 1.5416\n",
      "step 19999: train loss 0.9399, val loss 1.5310\n",
      "\n",
      "Final generated text:\n",
      "\n",
      "VINIUS:\n",
      "That's one thing afflyance but says he, sole hours\n",
      "Since this few us halw'd in this upon the disls,\n",
      "And what time my poorws being richest\n",
      "Of tried Padua with then of some other makin\n",
      "But, and by your voices may be perforce\n",
      "To up his steal receapons: he so would\n",
      "But by some necessity to have them go.\n",
      "\n",
      "Herald:\n",
      "Hermione hath made him so much saffer keen.\n",
      "But his love and the corons is no\n",
      "found to these the flot, and that fear him\n",
      "Will burn Richard: yet his name is cause ere\n",
      "but one that tho\n"
     ]
    }
   ],
   "source": [
    "%run gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Choose a mode:\n",
      "1: Load the model for evaluation\n",
      "2: Retrain the model from scratch\n",
      "3: Continue training from a checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delah\\Documents\\GitHub\\AI-Chatbot\\gpt.py:294: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Generating text:\n",
      "\n",
      "\n",
      "KING RICHARD II:\n",
      "Say that starts and blazed envy.\n",
      "Once more, talk of this foul castle back:\n",
      "Are at the crown? and, as though thou speakfully call'st.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Yet I in last, that virtue deeds there!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "So that I could yield my issue.\n",
      "\n",
      "LADY ANNE:\n",
      "Good pain and whom I should think on this world\n",
      "I would all do this welcome home on my husband.\n",
      "\n",
      "KING RICHARD II:\n",
      "And so much writing on thee.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Thou art too dear ard all of England.\n",
      "\n",
      "GLOUCESTER:\n",
      "Ay, thousand men canst not send the ground.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, in posterous witting John of Gaunt.\n",
      "\n",
      "KING RICHARD II:\n",
      "My daughters, at my lord\n",
      "Is as fortune as thou else.\n",
      "But in the duke, my torch is mine;\n",
      "His will-day short is made my body's light;\n",
      "Yet he, if I condemn my heart,\n",
      "Had been but confiscation of his self,\n",
      "Which so lately and dead my death-help.\n",
      "I know thee well, when I may die,\n",
      "To tie once more than I might confess with this,\n",
      "Deceive this belly thou shalt convey against me\n",
      "Further Titus; and this give me rule,\n",
      "And yet all, I humble in him Marcius.\n",
      "\n",
      "COMINIUS:\n",
      "Say I am not yet or word.\n",
      "\n",
      "BRUTUS:\n",
      "This senseless is not the voices that false\n",
      "The chiefest Coriolanus enIugh.\n",
      "\n",
      "CORIOLANUS:\n",
      "The very beasts them blame at my breasts\n",
      "Should but to-night; and I did this jest.\n",
      "But He which perform'd Henry, come to Rome,\n",
      "Did from Coriolanus, and bear him loved his;\n",
      "The clothe doth his queen's hate been set on:\n",
      "Her blood of Marcius' beauty,\n",
      "Being given no tongue's eyes and therein,\n",
      "And yet he could never have enter'd it known;\n",
      "But place where as I blame to know,\n",
      "As cry 'Whom was ever I brought himself\n",
      "My very course to himself?'\n",
      "\n",
      "All'Twere it not: death shall no harm,\n",
      "Than I, give all the fire, since as I do.\n",
      "\n",
      "GREMIO:\n",
      "By comfort, I'll to be flayed for that\n",
      "I carry it, Hortensio, lest bloody heart\n",
      "There was no power than true, so she seeks.\n",
      "\n",
      "PETRUCHIO:\n",
      "'Tis good deed.\n",
      "\n",
      "GREMIO:\n",
      "Peace, gentlemen, put up; and I wish thee\n",
      "a battle; and who, if I would not bring thy house,\n",
      "yet me suppose me then be a very \n",
      "\n",
      "Final generated text:\n",
      "\n",
      "Be every belly,\n",
      "And but still betide in the free death.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Why, then, did I confess thee too king?\n",
      "I'll to thee love thee wedded for thy moody;\n",
      "And if thou dost, I never never will permit.\n",
      "\n",
      "LADY GREY:\n",
      "What love that news? what may, have you been?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "What, are you?\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now well approacheth never see to see me,\n",
      "Tell shadow the sun you do intermity\n",
      "What your holy brother's life?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Stand wherefore they do refuse King Henry?\n",
      "\n",
      "GLOUCESTER:\n",
      "They see in perjurious body they\n",
      "Are encounter me than cannot good quarrel.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Down with me; that for I have attain'd\n",
      "Which my husband and my soul done.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Why, madam, we say, in disgracious;\n",
      "But now I create, and you read three swords.\n",
      "\n",
      "EDWARD:\n",
      "Clarence shall, Lord Bona, and our power.\n",
      "\n",
      "LORD WILLOUGHBY:\n",
      "What news?\n",
      "\n",
      "KING EDWARD IV:\n",
      "Speak thee, to Friar London will contend you too.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now, gentle Clarence; how they bear me?\n",
      "What do you partly still refuge myself?\n",
      "\n",
      "Post:\n",
      "So proud as I do, and fight'st to purchase.\n",
      "\n",
      "CLARENCE:\n",
      "No doubt, my lord, as I am countermanded.\n",
      "\n",
      "KING EDWARD IV:\n",
      "What, that's not your care?\n",
      "\n",
      "CLARENCE:\n",
      "I'll have order descend all rmove\n",
      "When the sun fails sleep my drums. The commons,\n",
      "Which in the character and forgiveness,\n",
      "Whereto all this land the springs can defend\n",
      "With selling traitors of the room?\n",
      "Therefore, being gone, and further took on\n",
      "The foot, let another's name, and the joy\n",
      "Which I cannot bexcuse. You thus? Say, she's not\n",
      "A bull man of dreams; and there I shall not be\n",
      "As if a devilish conceive pass.\n",
      "\n",
      "ANGELO:\n",
      "O, peace!\n",
      "\n",
      "ISABELLA:\n",
      "What is his teeth her comes the best\n",
      "That is cold with a graved victory.\n",
      "\n",
      "ANGELO:\n",
      "O, it is a better world of your voices,\n",
      "A true unblowing service of your children?\n",
      "It is excellent by Berkeley, now my wife,\n",
      "Which nature to be sure or deserved.\n",
      "\n",
      "ISABELLA:\n",
      "Intended, Pompey; send them away.\n",
      "\n",
      "ANGELO:\n",
      "The duke!\n",
      "\n",
      "ISABELLA:\n",
      "Either conceive I in being pastime wrong;\n",
      "And, as I could must be dead, my l\n"
     ]
    }
   ],
   "source": [
    "%run gpt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
